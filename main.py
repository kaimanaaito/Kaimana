"""
EDA Streamlit Pro â€” Polars + Streamlit + AutoML (å®Œå…¨ç‰ˆ)

Features:
- Polars-based fast CSV handling (lazy scan + sampling)
- Interactive Plotly visualizations with polished styling
- Automatic type inference and recommended statistical tests
- t-tests, Mann-Whitney U, ANOVA, chi-square
- PCA and KMeans clustering + visualizations
- Feature Engineering (OneHot, Text features, DateTime, Interactions)
- Outlier detection (IQR + Z-score)
- Intuitive file-merge UI with preview and download
- PDF & Markdown report generation (uses kaleido + fpdf)
- **NEW: AutoML with LightGBM, XGBoost, RandomForest**
- **NEW: Optuna hyperparameter optimization**
- **NEW: SHAP interpretability**
- **NEW: Model comparison dashboard**
- Caching for performance and responsive behavior for large files

Run:
pip install streamlit polars pandas numpy scipy scikit-learn plotly fpdf kaleido lightgbm xgboost optuna shap joblib
streamlit run eda_streamlit_pro_automl.py

Notes:
- For very large files the app uses smart sampling for plotting and exploratory summaries.
- Feature engineering allows analysis of any data type (text, categorical, numeric)
- AutoML supports both regression and classification with automatic task detection
"""

import streamlit as st
import polars as pl
import pandas as pd
import numpy as np
from io import StringIO, BytesIO
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit, cross_validate, train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, average_precision_score,
    mean_squared_error, mean_absolute_error, r2_score,
    classification_report, confusion_matrix
)
from sklearn.inspection import permutation_importance
import plotly.express as px
import plotly.graph_objects as go
import base64
import tempfile
import os
import time
import pickle
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# For PDF export
from fpdf import FPDF

# === AutoML extension ===
import lightgbm as lgb
import xgboost as xgb
import optuna
import shap
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

# Streamlit page config
st.set_page_config(page_title="EDA Studio Pro + AutoML", layout="wide", initial_sidebar_state='expanded')

# ---------------------- Styles & utilities ----------------------
st.markdown("""
<style>
/* Glassy container */
[data-testid='stAppViewContainer'] { background: linear-gradient(135deg, #0f172a 0%, #1f2937 100%); }
.stButton>button { background: linear-gradient(90deg,#8b5cf6,#ec4899); color: white; border: none; }
.metric-container { 
    background: rgba(139, 92, 246, 0.1); 
    padding: 1rem; 
    border-radius: 10px; 
    border: 1px solid rgba(139, 92, 246, 0.3);
}
</style>
""", unsafe_allow_html=True)

@st.cache_data
def read_csv_polars(file_buf, use_lazy=True, sample_rows=20000):
    file_buf.seek(0)
    try:
        if use_lazy:
            lf = pl.scan_csv(file_buf)
            sample = lf.limit(sample_rows).collect()
            pl_df = sample
            total_rows = None
            try:
                total_rows = lf.collect().height
                pl_df = lf.collect()
            except Exception:
                total_rows = None
        else:
            pl_df = pl.read_csv(file_buf)
            total_rows = pl_df.height
    except Exception as e:
        # fallback to pandas
        file_buf.seek(0)
        pdf = pd.read_csv(file_buf)
        pl_df = pl.from_pandas(pdf)
        total_rows = pl_df.height

    # create sampled pandas for plotting
    try:
        n = pl_df.height
        if n > 20000:
            sampled = pl_df.sample(n=20000, with_replacement=False)
        else:
            sampled = pl_df
        sampled_pd = sampled.to_pandas()
    except Exception:
        sampled_pd = pl_df.head(1000).to_pandas()

    return pl_df, sampled_pd

@st.cache_data
def infer_column_types(sampled_pd):
    types = {}
    for c in sampled_pd.columns:
        s = sampled_pd[c]
        if pd.api.types.is_numeric_dtype(s):
            types[c] = 'numeric'
        elif pd.api.types.is_datetime64_any_dtype(s):
            types[c] = 'datetime'
        else:
            # treat string-like but with low unique count as categorical
            nunique = s.nunique(dropna=True)
            if nunique <= min(50, max(10, int(len(s)*0.05))):
                types[c] = 'categorical'
            else:
                types[c] = 'text'
    return types

@st.cache_data
def compute_numeric_stats(arr):
    a = np.array(arr, dtype=float)
    a = a[~np.isnan(a)]
    if a.size == 0:
        return None
    return {
        'count': int(a.size),
        'mean': float(np.mean(a)),
        'std': float(np.std(a, ddof=0)),
        'median': float(np.median(a)),
        'min': float(np.min(a)),
        'max': float(np.max(a)),
        'q1': float(np.quantile(a,0.25)),
        'q3': float(np.quantile(a,0.75))
    }

@st.cache_data
def compute_basic_stats(sampled_pd, types):
    stats_dict = {}
    for c,t in types.items():
        if t == 'numeric':
            stats_dict[c] = compute_numeric_stats(sampled_pd[c].dropna())
        elif t in ('categorical','text'):
            vc = sampled_pd[c].value_counts().head(100)
            stats_dict[c] = { 
                'count': int(sampled_pd[c].notna().sum()), 
                'unique': int(sampled_pd[c].nunique(dropna=True)), 
                'top': vc.index[0] if vc.shape[0]>0 else None, 
                'value_counts': vc
            }
        else:
            stats_dict[c] = { 'info': 'datetime or other' }
    return stats_dict

# OneHot Encoding and Feature Engineering utilities
@st.cache_data
def create_onehot_features(sampled_pd, categorical_cols, max_categories=10):
    """Create OneHot encoded features from categorical columns"""
    encoded_df = sampled_pd.copy()
    encoding_info = {}
    
    for col in categorical_cols:
        # Get top categories to avoid too many dummy variables
        top_categories = sampled_pd[col].value_counts().head(max_categories).index.tolist()
        
        # Create dummy variables for top categories
        dummies = pd.get_dummies(sampled_pd[col], prefix=f'{col}', prefix_sep='_')
        
        # Only keep top categories
        dummy_cols = [f'{col}_{cat}' for cat in top_categories if f'{col}_{cat}' in dummies.columns]
        selected_dummies = dummies[dummy_cols]
        
        # Merge with main dataframe
        encoded_df = pd.concat([encoded_df, selected_dummies], axis=1)
        
        encoding_info[col] = {
            'original_unique': sampled_pd[col].nunique(),
            'encoded_cols': dummy_cols,
            'top_categories': top_categories
        }
    
    return encoded_df, encoding_info

@st.cache_data
def create_numeric_from_text(sampled_pd, text_cols):
    """Create numeric features from text columns"""
    numeric_features = {}
    
    for col in text_cols:
        # Length of text
        numeric_features[f'{col}_length'] = sampled_pd[col].astype(str).str.len()
        
        # Word count
        numeric_features[f'{col}_word_count'] = sampled_pd[col].astype(str).str.split().str.len()
        
        # Number of unique characters
        numeric_features[f'{col}_unique_chars'] = sampled_pd[col].astype(str).apply(lambda x: len(set(x)))
        
        # Number of digits
        numeric_features[f'{col}_digit_count'] = sampled_pd[col].astype(str).str.count(r'\d')
        
        # Number of uppercase letters
        numeric_features[f'{col}_upper_count'] = sampled_pd[col].astype(str).str.count(r'[A-Z]')
        
        # Contains specific patterns (email, URL, etc.)
        numeric_features[f'{col}_has_email'] = sampled_pd[col].astype(str).str.contains(r'@', case=False, na=False).astype(int)
        numeric_features[f'{col}_has_url'] = sampled_pd[col].astype(str).str.contains(r'http', case=False, na=False).astype(int)
    
    return pd.DataFrame(numeric_features)

@st.cache_data
def create_datetime_features(sampled_pd, datetime_cols):
    """Create numeric features from datetime columns"""
    datetime_features = {}
    
    for col in datetime_cols:
        # Convert to datetime if not already
        try:
            dt_series = pd.to_datetime(sampled_pd[col], errors='coerce')
        except:
            continue
            
        # Extract various datetime components
        datetime_features[f'{col}_year'] = dt_series.dt.year
        datetime_features[f'{col}_month'] = dt_series.dt.month
        datetime_features[f'{col}_day'] = dt_series.dt.day
        datetime_features[f'{col}_dayofweek'] = dt_series.dt.dayofweek
        datetime_features[f'{col}_hour'] = dt_series.dt.hour
        datetime_features[f'{col}_quarter'] = dt_series.dt.quarter
        datetime_features[f'{col}_is_weekend'] = (dt_series.dt.dayofweek >= 5).astype(int)
        
        # Time-based calculations
        reference_date = dt_series.min()
        datetime_features[f'{col}_days_since_start'] = (dt_series - reference_date).dt.days
    
    return pd.DataFrame(datetime_features)

@st.cache_data
def create_interaction_features(sampled_pd, numeric_cols, max_interactions=10):
    """Create interaction features between numeric columns"""
    interaction_features = {}
    
    # Limit to prevent too many features
    limited_cols = numeric_cols[:max_interactions]
    
    for i, col1 in enumerate(limited_cols):
        for col2 in limited_cols[i+1:]:
            # Multiplication
            interaction_features[f'{col1}_x_{col2}'] = sampled_pd[col1] * sampled_pd[col2]
            
            # Division (with protection against division by zero)
            with np.errstate(divide='ignore', invalid='ignore'):
                division = sampled_pd[col1] / (sampled_pd[col2] + 1e-8)
                division = np.where(np.isfinite(division), division, 0)
                interaction_features[f'{col1}_div_{col2}'] = division
            
            # Addition
            interaction_features[f'{col1}_plus_{col2}'] = sampled_pd[col1] + sampled_pd[col2]
            
            # Subtraction
            interaction_features[f'{col1}_minus_{col2}'] = sampled_pd[col1] - sampled_pd[col2]
    
    return pd.DataFrame(interaction_features)

def create_binned_features(sampled_pd, numeric_cols, n_bins=5):
    """Create categorical features by binning numeric columns"""
    binned_features = {}
    
    for col in numeric_cols:
        try:
            # Use quantile-based binning
            binned_features[f'{col}_binned'] = pd.qcut(
                sampled_pd[col], 
                q=n_bins, 
                labels=[f'{col}_bin_{i}' for i in range(n_bins)],
                duplicates='drop'
            )
        except:
            # Fallback to equal-width binning
            binned_features[f'{col}_binned'] = pd.cut(
                sampled_pd[col], 
                bins=n_bins, 
                labels=[f'{col}_bin_{i}' for i in range(n_bins)]
            )
    
    return pd.DataFrame(binned_features)

# Suggest appropriate tests based on types
def suggest_tests_for_columns(types, stats_dict):
    suggestions = []
    # find binary categorical vs numeric combos
    numeric_cols = [c for c,t in types.items() if t=='numeric']
    cat_cols = [c for c,t in types.items() if t=='categorical']

    for n in numeric_cols:
        for g in cat_cols:
            if g in stats_dict and 'unique' in stats_dict[g] and stats_dict[g]['unique'] == 2:
                suggestions.append({'test': 't-test (independent)', 'numeric': n, 'group': g, 'reason': 'binary group'})
            elif g in stats_dict and 'unique' in stats_dict[g]:
                suggestions.append({'test': 'ANOVA (1-way)', 'numeric': n, 'group': g, 'reason': 'categorical with >2 groups'})

    # categorical vs categorical
    if len(cat_cols) >= 2:
        for i in range(len(cat_cols)):
            for j in range(i+1,len(cat_cols)):
                suggestions.append({'test': 'chi-square', 'cat1': cat_cols[i], 'cat2': cat_cols[j]})
    return suggestions

# PCA & clustering utilities
@st.cache_data
def run_pca(sampled_pd, numeric_cols, n_components=3):
    X = sampled_pd[numeric_cols].dropna()
    if X.shape[0] == 0:
        return None
    pca = PCA(n_components=min(n_components, X.shape[1]))
    comps = pca.fit_transform(X)
    variance = pca.explained_variance_ratio_
    comp_df = pd.DataFrame(comps, columns=[f'PC{i+1}' for i in range(comps.shape[1])])
    return comp_df, variance

@st.cache_data
def run_kmeans(sampled_pd, numeric_cols, n_clusters=3):
    X = sampled_pd[numeric_cols].dropna()
    if X.shape[0] == 0:
        return None
    k = KMeans(n_clusters=n_clusters, random_state=42)
    labels = k.fit_predict(X)
    return labels

# Feature importance (light)
@st.cache_data
def compute_feature_importance(sampled_pd, target_col, numeric_cols):
    # train a small RF to get importances (classification)
    df = sampled_pd[numeric_cols + [target_col]].dropna()
    if df.shape[0] < 20:
        return None
    X = df[numeric_cols]
    y = df[target_col]
    # if numeric target, bin to classify
    if pd.api.types.is_numeric_dtype(y):
        y = pd.qcut(y, q=3, labels=False, duplicates='drop')
    try:
        rf = RandomForestClassifier(n_estimators=50, random_state=42)
        rf.fit(X, y)
        importances = pd.Series(rf.feature_importances_, index=numeric_cols).sort_values(ascending=False)
        return importances
    except Exception:
        return None

# === AutoML extension starts here ===

# AutoML preprocessing utilities
def preprocess_for_automl(df, target_col, task_type='auto'):
    """Comprehensive preprocessing for AutoML"""
    df_processed = df.copy()
    
    # Separate features and target
    X = df_processed.drop(columns=[target_col])
    y = df_processed[target_col]
    
    # Handle missing values in target
    mask = ~y.isnull()
    X = X.loc[mask]
    y = y.loc[mask]
    
    # Determine task type automatically if not specified
    if task_type == 'auto':
        if pd.api.types.is_numeric_dtype(y):
            unique_ratio = y.nunique() / len(y)
            if unique_ratio < 0.05 or y.nunique() <= 20:
                task_type = 'classification'
            else:
                task_type = 'regression'
        else:
            task_type = 'classification'
    
    # Encode target for classification
    target_encoder = None
    if task_type == 'classification' and not pd.api.types.is_numeric_dtype(y):
        target_encoder = LabelEncoder()
        y = target_encoder.fit_transform(y.astype(str))
    
    # Preprocessing pipeline
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    
    preprocessing_info = {
        'numeric_cols': numeric_cols,
        'categorical_cols': categorical_cols,
        'task_type': task_type,
        'target_encoder': target_encoder
    }
    
    # Handle numeric columns
    if numeric_cols:
        # Fill numeric missing values
        numeric_imputer = SimpleImputer(strategy='median')
        X[numeric_cols] = numeric_imputer.fit_transform(X[numeric_cols])
        preprocessing_info['numeric_imputer'] = numeric_imputer
        
        # Scale numeric features
        scaler = StandardScaler()
        X[numeric_cols] = scaler.fit_transform(X[numeric_cols])
        preprocessing_info['scaler'] = scaler
    
    # Handle categorical columns
    if categorical_cols:
        # Fill categorical missing values
        categorical_imputer = SimpleImputer(strategy='most_frequent')
        X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])
        
        # One-hot encode categorical features (limit to top categories)
        for col in categorical_cols:
            # Limit to top 10 categories to prevent explosion
            top_categories = X[col].value_counts().head(10).index
            X[col] = X[col].apply(lambda x: x if x in top_categories else 'OTHER')
        
        # One-hot encode
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        encoded = encoder.fit_transform(X[categorical_cols])
        encoded_feature_names = encoder.get_feature_names_out(categorical_cols)
        encoded_df = pd.DataFrame(encoded, columns=encoded_feature_names, index=X.index)
        
        # Replace categorical columns with encoded ones
        X = X.drop(columns=categorical_cols)
        X = pd.concat([X, encoded_df], axis=1)
        
        preprocessing_info['categorical_imputer'] = categorical_imputer
        preprocessing_info['encoder'] = encoder
    
    return X, y, preprocessing_info

def get_cv_strategy(cv_type, n_splits, y, task_type):
    """Get cross-validation strategy"""
    if cv_type == 'StratifiedKFold' and task_type == 'classification':
        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    elif cv_type == 'TimeSeriesSplit':
        return TimeSeriesSplit(n_splits=n_splits)
    else:
        return KFold(n_splits=n_splits, shuffle=True, random_state=42)

def create_models():
    """Create model instances"""
    models = {
        'LightGBM': {
            'model_class': lgb.LGBMRegressor,
            'classifier_class': lgb.LGBMClassifier,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'num_leaves': [31, 50, 100],
                'feature_fraction': [0.8, 0.9, 1.0],
                'bagging_fraction': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 1],
                'reg_lambda': [0, 0.1, 1]
            }
        },
        'XGBoost': {
            'model_class': xgb.XGBRegressor,
            'classifier_class': xgb.XGBClassifier,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 6, 10],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 1],
                'reg_lambda': [0, 0.1, 1]
            }
        },
        'RandomForest': {
            'model_class': RandomForestRegressor,
            'classifier_class': RandomForestClassifier,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            }
        }
    }
    return models

def create_optuna_objective(model_class, X_train, y_train, cv_strategy, task_type, param_space):
    """Create Optuna objective function"""
    def objective(trial):
        # Sample hyperparameters
        params = {}
        for param, values in param_space.items():
            if isinstance(values, list):
                if all(isinstance(v, int) for v in values):
                    params[param] = trial.suggest_int(param, min(values), max(values))
                elif all(isinstance(v, float) for v in values):
                    params[param] = trial.suggest_float(param, min(values), max(values))
                else:
                    params[param] = trial.suggest_categorical(param, values)
        
        # Add common parameters
        params['random_state'] = 42
        if 'LGB' in str(model_class):
            params['verbose'] = -1
            params['force_col_wise'] = True
        elif 'XGB' in str(model_class):
            params['verbosity'] = 0
        elif 'RandomForest' in str(model_class):
            params['n_jobs'] = -1
        
        # Create model
        model = model_class(**params)
        
        # Cross-validation
        scoring = 'roc_auc' if task_type == 'classification' else 'neg_root_mean_squared_error'
        try:
            cv_results = cross_validate(
                model, X_train, y_train, 
                cv=cv_strategy, 
                scoring=scoring, 
                n_jobs=1,  # Avoid nested parallelism
                error_score='raise'
            )
            return np.mean(cv_results['test_score'])
        except Exception as e:
            return float('-inf')  # Return bad score on failure
    
    return objective

def run_automl_optimization(X_train, y_train, model_name, task_type, cv_type, n_splits, n_trials, timeout_minutes):
    """Run Optuna hyperparameter optimization"""
    models = create_models()
    model_info = models[model_name]
    
    # Select appropriate model class
    model_class = model_info['classifier_class'] if task_type == 'classification' else model_info['model_class']
    param_space = model_info['param_space']
    
    # Create CV strategy
    cv_strategy = get_cv_strategy(cv_type, n_splits, y_train, task_type)
    
    # Create and run Optuna study
    study = optuna.create_study(
        direction='maximize',
        sampler=TPESampler(seed=42),
        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )
    
    objective = create_optuna_objective(model_class, X_train, y_train, cv_strategy, task_type, param_space)
    
    study.optimize(
        objective,
        n_trials=n_trials,
        timeout=timeout_minutes * 60,
        show_progress_bar=False  # Will show custom progress in Streamlit
    )
    
    # Get best parameters
    best_params = study.best_params
    best_params['random_state'] = 42
    
    # Add model-specific parameters
    if 'LGB' in str(model_class):
        best_params['verbose'] = -1
        best_params['force_col_wise'] = True
    elif 'XGB' in str(model_class):
        best_params['verbosity'] = 0
    elif 'RandomForest' in str(model_class):
        best_params['n_jobs'] = -1
    
    # Train final model
    best_model = model_class(**best_params)
    best_model.fit(X_train, y_train)
    
    return {
        'model': best_model,
        'best_params': best_params,
        'best_score': study.best_value,
        'study': study
    }

def evaluate_model(model, X_test, y_test, task_type):
    """Evaluate model performance"""
    y_pred = model.predict(X_test)
    
    results = {}
    
    if task_type == 'classification':
        results['accuracy'] = accuracy_score(y_test, y_pred)
        results['f1'] = f1_score(y_test, y_pred, average='weighted')
        
        # ROC-AUC and PR-AUC for binary classification
        if len(np.unique(y_test)) == 2:
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            results['roc_auc'] = roc_auc_score(y_test, y_pred_proba)
            results['pr_auc'] = average_precision_score(y_test, y_pred_proba)
        
        results['classification_report'] = classification_report(y_test, y_pred, output_dict=True)
        results['confusion_matrix'] = confusion_matrix(y_test, y_pred)
        
    else:  # regression
        results['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))
        results['mae'] = mean_absolute_error(y_test, y_pred)
        results['r2'] = r2_score(y_test, y_pred)
    
    results['predictions'] = y_pred
    
    return results

def compute_feature_importance_automl(model, feature_names, importance_type='gain'):
    """Compute feature importance for AutoML models"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_)
    else:
        return None
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    return importance_df

def compute_shap_values(model, X_sample, max_samples=100):
    """Compute SHAP values for model interpretation"""
    try:
        # Limit sample size for performance
        if len(X_sample) > max_samples:
            X_sample = X_sample.sample(n=max_samples, random_state=42)
        
        # Create SHAP explainer
        if hasattr(model, 'predict_proba'):  # Classification
            explainer = shap.TreeExplainer(model)
        else:  # Regression or other
            explainer = shap.TreeExplainer(model)
        
        shap_values = explainer.shap_values(X_sample)
        
        # Handle different SHAP value formats
        if isinstance(shap_values, list):
            # Multi-class classification case
            if len(shap_values) > 1:
                # Use positive class for binary classification
                shap_values = shap_values[1]
            else:
                shap_values = shap_values[0]
        
        # Ensure 2D shape
        if len(shap_values.shape) == 3:
            # If still 3D, take first dimension
            shap_values = shap_values[0]
        
        # Final validation
        if len(shap_values.shape) != 2:
            st.warning(f"SHAPå€¤ã®å½¢çŠ¶ãŒäºˆæœŸã—ã¾ã›ã‚“: {shap_values.shape}")
            return None, None, None
            
        if shap_values.shape[0] != len(X_sample) or shap_values.shape[1] != len(X_sample.columns):
            st.warning(f"SHAPå€¤ã®æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“: {shap_values.shape} vs expected {(len(X_sample), len(X_sample.columns))}")
            return None, None, None
        
        return shap_values, explainer, X_sample
        
    except Exception as e:
        st.warning(f"SHAPè¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        # Additional debugging info
        if 'shap_values' in locals():
            st.warning(f"SHAPå€¤ã®å½¢çŠ¶: {shap_values.shape if hasattr(shap_values, 'shape') else type(shap_values)}")
        return None, None, None

# Report generation: create pictures from plotly via to_image (requires kaleido)
def plotly_to_png(fig):
    try:
        img_bytes = fig.to_image(format='png', engine='kaleido')
        return img_bytes
    except Exception as e:
        return None

def create_pdf_report(title, description, images_with_captions):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font('Arial', 'B', 16)
    pdf.cell(0, 10, title, ln=True)
    pdf.set_font('Arial', '', 12)
    pdf.multi_cell(0, 8, description)
    for img_bytes, caption in images_with_captions:
        if img_bytes is None:
            continue
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
        tmp.write(img_bytes)
        tmp.close()
        pdf.add_page()
        pdf.image(tmp.name, x=10, y=20, w=190)
        pdf.ln(95)
        pdf.set_font('Arial', '', 11)
        pdf.multi_cell(0, 6, caption)
        os.unlink(tmp.name)
    out = BytesIO()
    pdf.output(out)
    out.seek(0)
    return out

# ---------------------- App layout ----------------------
st.title('EDA Studio Pro + AutoML â€” Polars + Streamlit')
st.caption('Fast, polished EDA and statistical analysis with AutoML capabilities')

left_col, right_col = st.columns([3,1])

# Sidebar controls
with right_col:
    st.header('è¨­å®š')
    use_lazy = st.checkbox('Lazyèª­ã¿è¾¼ã¿ (Polars scan_csv)', value=True)
    sample_size = st.number_input('ã‚µãƒ³ãƒ—ãƒ«è¡Œæ•° (ãƒ—ãƒ­ãƒƒãƒˆ)', min_value=2000, max_value=200000, value=20000, step=1000)
    display_rows = st.number_input('ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡Œæ•°', min_value=20, max_value=1000, value=200)
    
    st.markdown('---')
    st.write('ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ')
    uploaded = st.file_uploader('CSV ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰', type=['csv'], accept_multiple_files=True)
    st.write('ã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨')
    if st.button('ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ'):
        sample_pdf = pd.DataFrame({
            'age': np.random.randint(22,65,5000),
            'income': (np.random.normal(500000,120000,5000)).astype(int),
            'experience': np.random.randint(0,30,5000),
            'education': np.random.choice(['HS','BSc','MSc','PhD'],5000),
            'group': np.random.choice(['A','B','C'],5000),
            'outcome': np.random.choice([0,1],5000,p=[0.8,0.2])
        })
        buf = StringIO()
        sample_pdf.to_csv(buf, index=False)
        buf.seek(0)
        st.session_state['uploaded_sample'] = buf.getvalue()

    # === AutoML Sidebar Controls ===
    st.markdown('---')
    st.write('ğŸ¤– AutoMLè¨­å®š')
    
    # Initialize session state for AutoML if not exists
    if 'automl_target' not in st.session_state:
        st.session_state['automl_target'] = None

# load files
file_store = {}
if uploaded:
    for f in uploaded:
        try:
            pl_df, sampled_pd = read_csv_polars(f, use_lazy=use_lazy, sample_rows=sample_size)
            file_store[f.name] = (pl_df, sampled_pd)
        except Exception as e:
            st.error(f'{f.name} èª­ã¿è¾¼ã¿å¤±æ•—: {e}')

if 'uploaded_sample' in st.session_state and not uploaded:
    buf = StringIO(st.session_state['uploaded_sample'])
    pl_df, sampled_pd = read_csv_polars(buf, use_lazy=False, sample_rows=sample_size)
    file_store['sample.csv'] = (pl_df, sampled_pd)

if not file_store:
    st.warning('CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„')
    st.stop()

# Select file to analyze (main area)
file_names = list(file_store.keys())
selected_file = left_col.selectbox('è§£æãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ', options=file_names)
pl_df, sampled_pd = file_store[selected_file]

types = infer_column_types(sampled_pd)
basic_stats = compute_basic_stats(sampled_pd, types)

# Main tabs - Added AutoML tab
tabs = left_col.tabs(['Overview','Visualization','Feature Engineering','Stat Tests','PCA & Clustering','AutoML','Merge & Export','Report'])

# -------- Overview --------
with tabs[0]:
    st.subheader('æ¦‚è¦')
    c1,c2,c3 = st.columns(3)
    c1.metric('è¡Œæ•° (ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º)', sampled_pd.shape[0])
    c2.metric('åˆ—æ•°', sampled_pd.shape[1])
    numeric_count = sum(1 for v in types.values() if v=='numeric')
    c3.metric('æ•°å€¤åˆ—', numeric_count)
    st.write('---')
    st.write('å…ˆé ­è¡Œï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰')
    st.dataframe(sampled_pd.head(display_rows))
    st.write('---')
    st.write('åˆ—ã‚¿ã‚¤ãƒ—ã‚µãƒãƒª')
    st.table(pd.Series(types).rename('inferred_type'))
    st.write('---')

# -------- Visualization --------
with tabs[1]:
    st.subheader('å¯è¦–åŒ–')
    col_choice = st.selectbox('åˆ—ã‚’é¸æŠ (å¯è¦–åŒ–)', options=sampled_pd.columns)
    if col_choice:
        if types[col_choice]=='numeric':
            fig = px.histogram(sampled_pd, x=col_choice, nbins=80, title=f'åˆ†å¸ƒ: {col_choice}', marginal='box')
            left_col.plotly_chart(fig, use_container_width=True)
            # box + outliers
            if col_choice in basic_stats and basic_stats[col_choice] is not None:
                q1 = basic_stats[col_choice]['q1']; q3 = basic_stats[col_choice]['q3']; iqr = q3 - q1
                lower = q1 - 1.5*iqr; upper = q3 + 1.5*iqr
                outliers = sampled_pd[(sampled_pd[col_choice] < lower) | (sampled_pd[col_choice] > upper)]
                st.write(f'å¤–ã‚Œå€¤ (IQRåŸºæº–) æ¨å®š: {len(outliers)}')
        else:
            vc = sampled_pd[col_choice].value_counts().reset_index()
            vc.columns = [col_choice, 'count']
            fig = px.bar(vc.head(100), x=col_choice, y='count', title=f'é »åº¦: {col_choice}')
            left_col.plotly_chart(fig, use_container_width=True)

    st.write('---')
    # correlation
    numeric_cols = [c for c,t in types.items() if t=='numeric']
    if len(numeric_cols) >= 2:
        corr = sampled_pd[numeric_cols].corr()
        figc = px.imshow(corr, text_auto='.2f', title='ç›¸é–¢è¡Œåˆ—')
        left_col.plotly_chart(figc, use_container_width=True)
        if left_col.button('æ•£å¸ƒå›³è¡Œåˆ— (ä¸Šä½6åˆ—)'):
            figm = px.scatter_matrix(sampled_pd[numeric_cols].sample(n=min(2000, sampled_pd.shape[0])), dimensions=numeric_cols[:6])
            left_col.plotly_chart(figm, use_container_width=True)

# -------- Feature Engineering --------
with tabs[2]:
    st.subheader('ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°')
    
    # Get column types
    numeric_cols = [c for c,t in types.items() if t=='numeric']
    categorical_cols = [c for c,t in types.items() if t=='categorical']
    text_cols = [c for c,t in types.items() if t=='text']
    datetime_cols = [c for c,t in types.items() if t=='datetime']
    
    st.write(f"ç¾åœ¨ã®æ§‹æˆ: æ•°å€¤åˆ— {len(numeric_cols)}å€‹, ã‚«ãƒ†ã‚´ãƒªåˆ— {len(categorical_cols)}å€‹, ãƒ†ã‚­ã‚¹ãƒˆåˆ— {len(text_cols)}å€‹, æ—¥æ™‚åˆ— {len(datetime_cols)}å€‹")
    
    # Feature engineering options
    feature_options = st.multiselect(
        'ä½œæˆã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠ:',
        [
            'OneHot Encoding (ã‚«ãƒ†ã‚´ãƒªâ†’æ•°å€¤)',
            'ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ (é•·ã•ã€å˜èªæ•°ãªã©)',
            'æ—¥æ™‚ç‰¹å¾´é‡ (å¹´ã€æœˆã€æ›œæ—¥ãªã©)', 
            'æ•°å€¤äº¤äº’ä½œç”¨ (æ›ã‘ç®—ã€å‰²ã‚Šç®—ãªã©)',
            'æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚° (æ•°å€¤â†’ã‚«ãƒ†ã‚´ãƒª)',
            'çµ±è¨ˆçš„ç‰¹å¾´é‡ (æ¨™æº–åŒ–ã€ãƒ©ãƒ³ã‚¯ä»˜ã‘ãªã©)'
        ]
    )
    
    new_features_df = sampled_pd.copy()
    feature_info = {}
    
    if 'OneHot Encoding (ã‚«ãƒ†ã‚´ãƒªâ†’æ•°å€¤)' in feature_options and categorical_cols:
        st.write('### OneHot Encoding')
        selected_cat_cols = st.multiselect('OneHotåŒ–ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ—:', categorical_cols, default=categorical_cols)
        max_categories = st.slider('ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®æœ€å¤§ãƒ€ãƒŸãƒ¼å¤‰æ•°æ•°:', min_value=3, max_value=20, value=10)
        
        if selected_cat_cols:
            encoded_df, encoding_info = create_onehot_features(sampled_pd, selected_cat_cols, max_categories)
            
            # Add new columns to the main dataframe
            new_cols = []
            for col_info in encoding_info.values():
                new_cols.extend(col_info['encoded_cols'])
            
            new_features_df = pd.concat([new_features_df, encoded_df[new_cols]], axis=1)
            feature_info['onehot'] = encoding_info
            
            st.write(f'è¿½åŠ ã•ã‚ŒãŸåˆ—æ•°: {len(new_cols)}')
            st.write('ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±:', encoding_info)
    
    if 'ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ (é•·ã•ã€å˜èªæ•°ãªã©)' in feature_options and text_cols:
        st.write('### ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡')
        selected_text_cols = st.multiselect('åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ—:', text_cols, default=text_cols)
        
        if selected_text_cols:
            text_features = create_numeric_from_text(sampled_pd, selected_text_cols)
            new_features_df = pd.concat([new_features_df, text_features], axis=1)
            feature_info['text'] = list(text_features.columns)
            st.write(f'è¿½åŠ ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡: {len(text_features.columns)}å€‹')
    
    if 'æ—¥æ™‚ç‰¹å¾´é‡ (å¹´ã€æœˆã€æ›œæ—¥ãªã©)' in feature_options and datetime_cols:
        st.write('### æ—¥æ™‚ç‰¹å¾´é‡')
        selected_dt_cols = st.multiselect('åˆ†æã™ã‚‹æ—¥æ™‚åˆ—:', datetime_cols, default=datetime_cols)
        
        if selected_dt_cols:
            dt_features = create_datetime_features(sampled_pd, selected_dt_cols)
            new_features_df = pd.concat([new_features_df, dt_features], axis=1)
            feature_info['datetime'] = list(dt_features.columns)
            st.write(f'è¿½åŠ ã•ã‚ŒãŸæ—¥æ™‚ç‰¹å¾´é‡: {len(dt_features.columns)}å€‹')
    
    if 'æ•°å€¤äº¤äº’ä½œç”¨ (æ›ã‘ç®—ã€å‰²ã‚Šç®—ãªã©)' in feature_options and len(numeric_cols) >= 2:
        st.write('### æ•°å€¤äº¤äº’ä½œç”¨ç‰¹å¾´é‡')
        max_interactions = st.slider('äº¤äº’ä½œç”¨ã‚’ä½œã‚‹æœ€å¤§åˆ—æ•°:', min_value=2, max_value=min(10, len(numeric_cols)), value=min(5, len(numeric_cols)))
        
        interaction_features = create_interaction_features(sampled_pd, numeric_cols, max_interactions)
        new_features_df = pd.concat([new_features_df, interaction_features], axis=1)
        feature_info['interactions'] = list(interaction_features.columns)
        st.write(f'è¿½åŠ ã•ã‚ŒãŸäº¤äº’ä½œç”¨ç‰¹å¾´é‡: {len(interaction_features.columns)}å€‹')
    
    if 'æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚° (æ•°å€¤â†’ã‚«ãƒ†ã‚´ãƒª)' in feature_options and numeric_cols:
        st.write('### æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚°')
        selected_num_cols = st.multiselect('ãƒ“ãƒ‹ãƒ³ã‚°ã™ã‚‹æ•°å€¤åˆ—:', numeric_cols)
        n_bins = st.slider('ãƒ“ãƒ³æ•°:', min_value=3, max_value=10, value=5)
        
        if selected_num_cols:
            binned_features = create_binned_features(sampled_pd, selected_num_cols, n_bins)
            new_features_df = pd.concat([new_features_df, binned_features], axis=1)
            feature_info['binned'] = list(binned_features.columns)
            st.write(f'è¿½åŠ ã•ã‚ŒãŸãƒ“ãƒ‹ãƒ³ã‚°ç‰¹å¾´é‡: {len(binned_features.columns)}å€‹')
    
    if 'çµ±è¨ˆçš„ç‰¹å¾´é‡ (æ¨™æº–åŒ–ã€ãƒ©ãƒ³ã‚¯ä»˜ã‘ãªã©)' in feature_options and numeric_cols:
        st.write('### çµ±è¨ˆçš„ç‰¹å¾´é‡')
        selected_stat_cols = st.multiselect('çµ±è¨ˆå‡¦ç†ã™ã‚‹æ•°å€¤åˆ—:', numeric_cols)
        
        if selected_stat_cols:
            stat_features = {}
            for col in selected_stat_cols:
                # Standardization
                stat_features[f'{col}_std'] = (sampled_pd[col] - sampled_pd[col].mean()) / sampled_pd[col].std()
                # Rank
                stat_features[f'{col}_rank'] = sampled_pd[col].rank()
                # Log transform (with protection)
                stat_features[f'{col}_log'] = np.log1p(np.abs(sampled_pd[col]))
                # Square root
                stat_features[f'{col}_sqrt'] = np.sqrt(np.abs(sampled_pd[col]))
            
            stat_df = pd.DataFrame(stat_features)
            new_features_df = pd.concat([new_features_df, stat_df], axis=1)
            feature_info['statistical'] = list(stat_df.columns)
            st.write(f'è¿½åŠ ã•ã‚ŒãŸçµ±è¨ˆçš„ç‰¹å¾´é‡: {len(stat_df.columns)}å€‹')
    
    # Show results
    if len(new_features_df.columns) > len(sampled_pd.columns):
        st.success(f'ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼ {len(sampled_pd.columns)} â†’ {len(new_features_df.columns)} åˆ—')
        
        # Update the dataframe in session state for other tabs to use
        st.session_state['engineered_df'] = new_features_df
        st.session_state['feature_info'] = feature_info
        
        # Show new feature statistics
        new_cols = [col for col in new_features_df.columns if col not in sampled_pd.columns]
        if st.checkbox('æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¡¨ç¤º', value=False):
            st.dataframe(new_features_df[new_cols].head(100))
        
        # Download engineered features
        csv_engineered = new_features_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
            data=csv_engineered,
            file_name=f'engineered_{selected_file}',
            mime='text/csv'
        )
        
        # Update types for new features
        if st.button('æ–°ã—ã„ç‰¹å¾´é‡ã§å‹æ¨å®šã‚’æ›´æ–°'):
            # Re-infer types for the new dataframe
            new_types = infer_column_types(new_features_df)
            st.session_state['engineered_types'] = new_types
            st.success('å‹æ¨å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ')
    else:
        st.info('ç‰¹å¾´é‡ã‚’é¸æŠã—ã¦ä½œæˆã—ã¦ãã ã•ã„')

# -------- Stat Tests --------
with tabs[3]:
    st.subheader('çµ±è¨ˆçš„æ¤œå®š & æ¨å¥¨æ¤œå®š')
    
    # Use engineered dataframe if available
    analysis_df = st.session_state.get('engineered_df', sampled_pd)
    analysis_types = st.session_state.get('engineered_types', types)
    
    # Get column types from the analysis dataframe
    numeric_cols = [c for c,t in analysis_types.items() if t=='numeric']
    categorical_cols = [c for c,t in analysis_types.items() if t=='categorical']
    
    st.write(f'è§£æå¯¾è±¡: {analysis_df.shape[1]}åˆ— (æ•°å€¤: {len(numeric_cols)}, ã‚«ãƒ†ã‚´ãƒª: {len(categorical_cols)})')
    
    # Re-compute basic stats for analysis dataframe
    analysis_stats = compute_basic_stats(analysis_df, analysis_types)
    
    suggestions = suggest_tests_for_columns(analysis_types, analysis_stats)
    st.write('ãŠã™ã™ã‚ã®æ¤œå®šï¼ˆè‡ªå‹•ææ¡ˆï¼‰')
    for s in suggestions[:10]:
        st.write(s)
    st.write('---')
    
    # Get available columns for each type
    if not numeric_cols:
        st.warning('æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¿ãƒ–ã§OneHotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚')
    if not categorical_cols:
        st.warning('ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¿ãƒ–ã§æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚°ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚')
    
    # manual test execution
    test_type = st.selectbox('æ¤œå®šã‚¿ã‚¤ãƒ—', ['t-test (indep)', 'paired t-test', 'ANOVA', 'Mann-Whitney U', 'Chi-square'])
    
    if test_type in ['t-test (indep)','paired t-test','ANOVA','Mann-Whitney U']:
        # Only show options if numeric and categorical columns exist
        if not numeric_cols:
            st.info('ğŸ’¡ ãƒ’ãƒ³ãƒˆ: OneHotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã§æ•°å€¤åˆ—ã‚’ä½œæˆã§ãã¾ã™')
        elif not categorical_cols:
            st.info('ğŸ’¡ ãƒ’ãƒ³ãƒˆ: æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚°ã§ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’ä½œæˆã§ãã¾ã™')
        else:
            num = st.selectbox('æ•°å€¤åˆ— (å¾“å±)', options=numeric_cols, key='numeric_col_selection_eng')
            grp = st.selectbox('ã‚°ãƒ«ãƒ¼ãƒ—åˆ— (ç‹¬ç«‹å¤‰æ•°)', options=categorical_cols, key='group_col_selection_eng')
            
            if st.button('æ¤œå®šã‚’å®Ÿè¡Œ') and num is not None and grp is not None:
                res = None
                if test_type=='t-test (indep)':
                    grp_df = analysis_df[[num,grp]].dropna()
                    groups = grp_df[grp].unique()
                    if len(groups)!=2:
                        st.error('ã‚°ãƒ«ãƒ¼ãƒ—ã¯2ã¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™')
                    else:
                        a = grp_df[grp_df[grp]==groups[0]][num]
                        b = grp_df[grp_df[grp]==groups[1]][num]
                        r = stats.ttest_ind(a,b, equal_var=False, nan_policy='omit')
                        res = {'stat': float(r.statistic), 'pvalue': float(r.pvalue)}
                elif test_type=='paired t-test':
                    st.info('å¯¾å¿œtæ¤œå®šã¯ãƒ‡ãƒ¼ã‚¿ã‚’æ•´åˆ—ã•ã›ãŸãƒšã‚¢ãŒå¿…è¦ã§ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ã¯ãƒšã‚¢ãŒãªã„å ´åˆã¯æœ€åˆã®Nã‚’ä½¿ã„ã¾ã™')
                    grp_df = analysis_df[[num,grp]].dropna()
                    groups = grp_df[grp].unique()
                    if len(groups)!=2:
                        st.error('ã‚°ãƒ«ãƒ¼ãƒ—ã¯2ã¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™')
                    else:
                        a = grp_df[grp_df[grp]==groups[0]][num].values
                        b = grp_df[grp_df[grp]==groups[1]][num].values
                        m = min(len(a), len(b))
                        if m == 0:
                            st.error('ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™')
                        else:
                            r = stats.ttest_rel(a[:m], b[:m], nan_policy='omit')
                            res = {'stat': float(r.statistic), 'pvalue': float(r.pvalue)}
                elif test_type=='ANOVA':
                    grp_df = analysis_df[[num,grp]].dropna()
                    groups = grp_df[grp].unique()
                    arrays = [grp_df[grp_df[grp]==g][num].values for g in groups]
                    r = stats.f_oneway(*arrays)
                    res = {'stat': float(r.statistic), 'pvalue': float(r.pvalue)}
                elif test_type=='Mann-Whitney U':
                    grp_df = analysis_df[[num,grp]].dropna()
                    groups = grp_df[grp].unique()
                    if len(groups)!=2:
                        st.error('ã‚°ãƒ«ãƒ¼ãƒ—ã¯2ã¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™')
                    else:
                        a = grp_df[grp_df[grp]==groups[0]][num]
                        b = grp_df[grp_df[grp]==groups[1]][num]
                        r = stats.mannwhitneyu(a,b, alternative='two-sided')
                        res = {'stat': float(r.statistic), 'pvalue': float(r.pvalue)}
                if res:
                    st.json(res)
                    st.write('p < 0.05 -> å¸°ç„¡ä»®èª¬ã‚’æ£„å´' if res.get('pvalue',1) < 0.05 else 'å¸°ç„¡ä»®èª¬ã‚’æ¡æŠ')
    else:
        # Chi-square test
        non_numeric_cols = [c for c,t in analysis_types.items() if t!='numeric']
        if len(non_numeric_cols) < 2:
            st.warning('ã‚«ã‚¤äºŒä¹—æ¤œå®šã«ã¯2ã¤ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒå¿…è¦ã§ã™ã€‚')
            st.info('ğŸ’¡ ãƒ’ãƒ³ãƒˆ: æ•°å€¤ãƒ“ãƒ‹ãƒ³ã‚°ã§ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’ä½œæˆã§ãã¾ã™')
        else:
            c1 = st.selectbox('ã‚«ãƒ†ã‚´ãƒªåˆ—1', options=non_numeric_cols, key='cat1_selection_eng')
            c2 = st.selectbox('ã‚«ãƒ†ã‚´ãƒªåˆ—2', options=[c for c in non_numeric_cols if c!=c1], key='cat2_selection_eng')
            if st.button('ã‚«ã‚¤äºŒä¹—æ¤œå®šå®Ÿè¡Œ') and c1 is not None and c2 is not None:
                try:
                    ct = pd.crosstab(analysis_df[c1], analysis_df[c2])
                    chi2, p, dof, ex = stats.chi2_contingency(ct)
                    st.json({'chi2':float(chi2), 'pvalue':float(p), 'dof':int(dof)})
                    st.write('p < 0.05 -> å¸°ç„¡ä»®èª¬ã‚’æ£„å´ (2ã¤ã®å¤‰æ•°ã«é–¢é€£ã‚ã‚Š)' if p < 0.05 else 'å¸°ç„¡ä»®èª¬ã‚’æ¡æŠ (2ã¤ã®å¤‰æ•°ã«é–¢é€£ãªã—)')
                except Exception as e:
                    st.error(f'ã‚«ã‚¤äºŒä¹—æ¤œå®šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')

# -------- PCA & Clustering --------
with tabs[4]:
    st.subheader('PCA & Clustering')
    
    # Use engineered dataframe if available
    analysis_df = st.session_state.get('engineered_df', sampled_pd)
    analysis_types = st.session_state.get('engineered_types', types)
    numeric_cols = [c for c,t in analysis_types.items() if t=='numeric']
    
    if len(numeric_cols) < 2:
        st.info('2ã¤ä»¥ä¸Šã®æ•°å€¤åˆ—ãŒå¿…è¦ã§ã™')
        st.info('ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¿ãƒ–ã§OneHotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ãã ã•ã„')
    else:
        st.write(f'ä½¿ç”¨å¯èƒ½ãªæ•°å€¤åˆ—: {len(numeric_cols)}å€‹')
        
        # Column selection for PCA
        selected_pca_cols = st.multiselect(
            'PCAã«ä½¿ç”¨ã™ã‚‹åˆ—ã‚’é¸æŠ:', 
            numeric_cols, 
            default=numeric_cols[:min(10, len(numeric_cols))]
        )
        
        if selected_pca_cols and len(selected_pca_cols) >= 2:
            n_comp = st.slider('ä¸»æˆåˆ†æ•°', min_value=2, max_value=min(6, len(selected_pca_cols)), value=min(3, len(selected_pca_cols)))
            pc_res = run_pca(analysis_df, selected_pca_cols, n_comp)
            if pc_res is not None:
                comp_df, var = pc_res
                comp_df_display = comp_df.copy()
                comp_df_display['index'] = np.arange(len(comp_df_display))
                
                # PCA scatter plot with better visualization
                fig = px.scatter(
                    comp_df_display, 
                    x='PC1', y='PC2', 
                    title=f'PCA: PC1 ({var[0]:.1%}) vs PC2 ({var[1]:.1%})',
                    labels={'PC1': f'PC1 ({var[0]:.1%})', 'PC2': f'PC2 ({var[1]:.1%})'}
                )
                st.plotly_chart(fig, use_container_width=True)
                st.write('å„ä¸»æˆåˆ†ã®å¯„ä¸ç‡:', [f'PC{i+1}: {float(v):.1%}' for i, v in enumerate(var)])
                st.write('ç´¯ç©å¯„ä¸ç‡:', f'{sum(var):.1%}')
                
            # Clustering
            st.write('### ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°')
            k = st.slider('ã‚¯ãƒ©ã‚¹ã‚¿æ•° (KMeans)', min_value=2, max_value=10, value=3)
            cluster_cols = st.multiselect(
                'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹åˆ—:', 
                numeric_cols, 
                default=selected_pca_cols[:min(5, len(selected_pca_cols))]
            )
            
            if cluster_cols:
                labels = run_kmeans(analysis_df, cluster_cols, k)
                if labels is not None:
                    # Create visualization dataframe
                    X = analysis_df[cluster_cols].dropna().copy()
                    X['cluster'] = labels
                    
                    # Scatter matrix for clusters
                    if len(cluster_cols) >= 2:
                        figc = px.scatter_matrix(
                            X.sample(n=min(2000, X.shape[0])), 
                            dimensions=cluster_cols[:4], 
                            color='cluster',
                            title=f'ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ (k={k})'
                        )
                        st.plotly_chart(figc, use_container_width=True)
                        
                        # Cluster statistics
                        cluster_stats = X.groupby('cluster')[cluster_cols].mean()
                        st.write('### ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥çµ±è¨ˆ')
                        st.dataframe(cluster_stats)
                        
                        # Add cluster labels to session state
                        analysis_df_with_clusters = analysis_df.copy()
                        analysis_df_with_clusters['cluster'] = np.nan
                        analysis_df_with_clusters.loc[X.index, 'cluster'] = labels
                        st.session_state['clustered_df'] = analysis_df_with_clusters

# ======== AutoML Tab ========
with tabs[5]:
    st.subheader('ğŸ¤– AutoML - è‡ªå‹•æ©Ÿæ¢°å­¦ç¿’')
    
    # Use engineered dataframe if available
    ml_df = st.session_state.get('engineered_df', sampled_pd)
    
    st.write(f'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {ml_df.shape[0]}è¡Œ, {ml_df.shape[1]}åˆ—')
    
    # Target column selection
    st.write('### 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®é¸æŠ')
    target_col = st.selectbox(
        'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆäºˆæ¸¬ã—ãŸã„å¤‰æ•°ï¼‰ã‚’é¸æŠ:',
        options=['é¸æŠã—ã¦ãã ã•ã„'] + list(ml_df.columns),
        key='target_selection'
    )
    
    if target_col == 'é¸æŠã—ã¦ãã ã•ã„':
        st.info('ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’é¸æŠã—ã¦AutoMLã‚’é–‹å§‹ã—ã¦ãã ã•ã„')
    else:
        # Display target information
        st.write('ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®çµ±è¨ˆæƒ…å ±:')
        target_series = ml_df[target_col]
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric('ãƒ‡ãƒ¼ã‚¿æ•°', target_series.notna().sum())
        with col2:
            st.metric('æ¬ æå€¤', target_series.isna().sum())
        with col3:
            st.metric('ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°', target_series.nunique())
        with col4:
            unique_ratio = target_series.nunique() / len(target_series)
            st.metric('ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡', f'{unique_ratio:.3f}')
        
        # Auto-detect task type
        if pd.api.types.is_numeric_dtype(target_series):
            if unique_ratio < 0.05 or target_series.nunique() <= 20:
                detected_task = 'classification'
            else:
                detected_task = 'regression'
        else:
            detected_task = 'classification'
        
        task_type = st.selectbox(
            'ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—:',
            options=['auto', 'classification', 'regression'],
            help=f'è‡ªå‹•æ¤œå‡º: {detected_task}'
        )
        
        if task_type == 'auto':
            task_type = detected_task
        
        st.info(f'é¸æŠã•ã‚ŒãŸã‚¿ã‚¹ã‚¯: **{task_type}**')
        
        # Model selection
        st.write('### 2. ãƒ¢ãƒ‡ãƒ«é¸æŠ')
        available_models = ['LightGBM', 'XGBoost', 'RandomForest']
        selected_models = st.multiselect(
            'ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«:',
            options=available_models,
            default=available_models
        )
        
        if not selected_models:
            st.warning('å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„')
        else:
            # Cross-validation settings
            st.write('### 3. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š')
            col1, col2 = st.columns(2)
            
            with col1:
                cv_type = st.selectbox(
                    'CVæ‰‹æ³•:',
                    options=['StratifiedKFold', 'KFold', 'TimeSeriesSplit'],
                    help='StratifiedKFold: åˆ†é¡ç”¨, KFold: å›å¸°ç”¨, TimeSeriesSplit: æ™‚ç³»åˆ—ç”¨'
                )
            
            with col2:
                n_splits = st.slider('åˆ†å‰²æ•°:', min_value=3, max_value=10, value=5)
            
            # Optuna settings
            st.write('### 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–è¨­å®š')
            col1, col2 = st.columns(2)
            
            with col1:
                n_trials = st.slider('è©¦è¡Œå›æ•°:', min_value=10, max_value=500, value=100)
            
            with col2:
                timeout_minutes = st.slider('åˆ¶é™æ™‚é–“ (åˆ†):', min_value=1, max_value=60, value=10)
            
            # Feature selection
            st.write('### 5. ç‰¹å¾´é‡é¸æŠ')
            feature_cols = [col for col in ml_df.columns if col != target_col]
            
            if st.checkbox('ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨', value=True):
                selected_features = feature_cols
            else:
                selected_features = st.multiselect(
                    'ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡:',
                    options=feature_cols,
                    default=feature_cols[:20]  # Limit default selection
                )
            
            st.write(f'é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(selected_features)}')
            
            # Start AutoML
            if st.button('ğŸš€ AutoMLå®Ÿè¡Œ', type='primary'):
                if not selected_features:
                    st.error('ç‰¹å¾´é‡ã‚’é¸æŠã—ã¦ãã ã•ã„')
                else:
                    with st.spinner('AutoMLã‚’å®Ÿè¡Œä¸­...'):
                        try:
                            # Preprocessing
                            st.info('å‰å‡¦ç†ä¸­...')
                            ml_subset = ml_df[selected_features + [target_col]].copy()
                            X, y, preprocessing_info = preprocess_for_automl(ml_subset, target_col, task_type)
                            
                            st.success(f'å‰å‡¦ç†å®Œäº†: {X.shape[1]}ç‰¹å¾´é‡, {len(y)}ã‚µãƒ³ãƒ—ãƒ«')
                            
                            # Train-test split
                            X_train, X_test, y_train, y_test = train_test_split(
                                X, y, test_size=0.2, random_state=42,
                                stratify=y if task_type == 'classification' else None
                            )
                            
                            # Store results
                            automl_results = {}
                            
                            # Progress tracking
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            # Train each model
                            for i, model_name in enumerate(selected_models):
                                status_text.text(f'æœ€é©åŒ–ä¸­: {model_name}...')
                                progress_bar.progress((i) / len(selected_models))
                                
                                start_time = time.time()
                                
                                # Run optimization
                                result = run_automl_optimization(
                                    X_train, y_train, model_name, task_type,
                                    cv_type, n_splits, n_trials, timeout_minutes
                                )
                                
                                training_time = time.time() - start_time
                                
                                # Evaluate on test set
                                test_results = evaluate_model(result['model'], X_test, y_test, task_type)
                                
                                # Compute feature importance
                                feature_importance = compute_feature_importance_automl(
                                    result['model'], X.columns
                                )
                                
                                # Store results
                                automl_results[model_name] = {
                                    'model': result['model'],
                                    'best_params': result['best_params'],
                                    'cv_score': result['best_score'],
                                    'test_results': test_results,
                                    'feature_importance': feature_importance,
                                    'training_time': training_time,
                                    'study': result['study']
                                }
                            
                            progress_bar.progress(1.0)
                            status_text.text('å®Œäº†!')
                            
                            # Store in session state
                            st.session_state['automl_results'] = automl_results
                            st.session_state['automl_task_type'] = task_type
                            st.session_state['automl_X_test'] = X_test
                            st.session_state['automl_y_test'] = y_test
                            st.session_state['automl_preprocessing'] = preprocessing_info
                            
                            st.success('AutoMLå®Œäº†!')
                            
                        except Exception as e:
                            st.error(f'AutoMLã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}')
                            import traceback
                            st.code(traceback.format_exc())

# Display AutoML results if available
if 'automl_results' in st.session_state:
    st.write('---')
    st.write('### ğŸ† ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ')
    
    results = st.session_state['automl_results']
    task_type = st.session_state['automl_task_type']
    
    # Create comparison table
    comparison_data = []
    for model_name, result in results.items():
        row = {
            'Model': model_name,
            'CV Score': f"{result['cv_score']:.4f}",
            'Training Time (s)': f"{result['training_time']:.1f}"
        }
        
        if task_type == 'classification':
            row['Test Accuracy'] = f"{result['test_results']['accuracy']:.4f}"
            row['Test F1'] = f"{result['test_results']['f1']:.4f}"
            if 'roc_auc' in result['test_results']:
                row['Test ROC-AUC'] = f"{result['test_results']['roc_auc']:.4f}"
        else:
            row['Test RMSE'] = f"{result['test_results']['rmse']:.4f}"
            row['Test MAE'] = f"{result['test_results']['mae']:.4f}"
            row['Test RÂ²'] = f"{result['test_results']['r2']:.4f}"
        
        comparison_data.append(row)
    
    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df, use_container_width=True)
    
    # Model selection for detailed analysis
    st.write('### ğŸ“Š è©³ç´°åˆ†æ')
    selected_model = st.selectbox(
        'è©³ç´°åˆ†æã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:',
        options=list(results.keys())
    )
    
    if selected_model:
        result = results[selected_model]
        
        # Performance metrics
        st.write(f'#### {selected_model} - æ€§èƒ½æŒ‡æ¨™')
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write('**ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ**')
            st.metric('CV Score', f"{result['cv_score']:.4f}")
            
            st.write('**æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**')
            st.json(result['best_params'])
        
        with col2:
            st.write('**ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆçµæœ**')
            test_results = result['test_results']
            
            if task_type == 'classification':
                col2_1, col2_2 = st.columns(2)
                with col2_1:
                    st.metric('Accuracy', f"{test_results['accuracy']:.4f}")
                    st.metric('F1 Score', f"{test_results['f1']:.4f}")
                with col2_2:
                    if 'roc_auc' in test_results:
                        st.metric('ROC-AUC', f"{test_results['roc_auc']:.4f}")
                    if 'pr_auc' in test_results:
                        st.metric('PR-AUC', f"{test_results['pr_auc']:.4f}")
                
                # Confusion Matrix
                if 'confusion_matrix' in test_results:
                    st.write('**æ··åŒè¡Œåˆ—**')
                    cm = test_results['confusion_matrix']
                    fig_cm = px.imshow(
                        cm, 
                        text_auto=True, 
                        title='Confusion Matrix',
                        labels=dict(x="Predicted", y="Actual")
                    )
                    st.plotly_chart(fig_cm, use_container_width=True)
            
            else:  # regression
                col2_1, col2_2 = st.columns(2)
                with col2_1:
                    st.metric('RMSE', f"{test_results['rmse']:.4f}")
                    st.metric('MAE', f"{test_results['mae']:.4f}")
                with col2_2:
                    st.metric('RÂ² Score', f"{test_results['r2']:.4f}")
                
                # Actual vs Predicted plot
                y_test = st.session_state['automl_y_test']
                y_pred = test_results['predictions']
                
                pred_df = pd.DataFrame({
                    'Actual': y_test,
                    'Predicted': y_pred
                })
                
                fig_pred = px.scatter(
                    pred_df, 
                    x='Actual', 
                    y='Predicted',
                    title='Actual vs Predicted',
                    trendline='ols'
                )
                # Add perfect prediction line
                min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())
                max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())
                fig_pred.add_shape(
                    type="line", line=dict(dash='dash'),
                    x0=min_val, y0=min_val, x1=max_val, y1=max_val
                )
                st.plotly_chart(fig_pred, use_container_width=True)
        
        # Feature importance
        st.write('#### ğŸ“ˆ ç‰¹å¾´é‡é‡è¦åº¦')
        if result['feature_importance'] is not None:
            importance_df = result['feature_importance'].head(20)
            
            fig_importance = px.bar(
                importance_df,
                y='feature',
                x='importance',
                orientation='h',
                title=f'{selected_model} - Top 20 Feature Importance'
            )
            fig_importance.update_layout(height=600)
            st.plotly_chart(fig_importance, use_container_width=True)
            
            # Download feature importance
            csv_importance = importance_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                'ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)',
                data=csv_importance,
                file_name=f'{selected_model}_feature_importance.csv',
                mime='text/csv'
            )
        else:
            st.info('ç‰¹å¾´é‡é‡è¦åº¦ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“')
        
        # SHAP Analysis
        st.write('#### ğŸ” SHAPè§£é‡ˆå¯èƒ½æ€§åˆ†æ')
        
        if st.button('SHAPåˆ†æã‚’å®Ÿè¡Œ'):
            with st.spinner('SHAPå€¤ã‚’è¨ˆç®—ä¸­...'):
                X_test = st.session_state['automl_X_test']
                shap_values, explainer, X_sample = compute_shap_values(
                    result['model'], X_test, max_samples=100
                )
                
                if shap_values is not None:
                    st.success('SHAPåˆ†æå®Œäº†!')
                    
                    # Handle different SHAP value shapes
                    # For multi-class classification, shap_values might be 3D
                    if len(shap_values.shape) == 3:
                        # Multi-class case: take the values for the first class or average
                        st.info(f'å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å½¢çŠ¶: {shap_values.shape}')
                        if task_type == 'classification':
                            # Use values for class 1 (positive class) or average across classes
                            if shap_values.shape[0] >= 2:
                                shap_values_2d = shap_values[1]  # Positive class
                                st.info('é™½æ€§ã‚¯ãƒ©ã‚¹ã®SHAPå€¤ã‚’ä½¿ç”¨ã—ã¾ã™')
                            else:
                                shap_values_2d = shap_values[0]  # First class
                                st.info('ç¬¬1ã‚¯ãƒ©ã‚¹ã®SHAPå€¤ã‚’ä½¿ç”¨ã—ã¾ã™')
                        else:
                            # For regression, average across the last dimension if needed
                            shap_values_2d = np.mean(shap_values, axis=0) if shap_values.shape[0] > 1 else shap_values[0]
                    elif len(shap_values.shape) == 2:
                        # Standard case: 2D array
                        shap_values_2d = shap_values
                    else:
                        st.error(f'äºˆæœŸã—ãªã„SHAPå€¤ã®å½¢çŠ¶: {shap_values.shape}')
                        st.stop()
                    
                    # Ensure we have the right shape
                    if shap_values_2d.shape[0] != len(X_sample) or shap_values_2d.shape[1] != len(X_sample.columns):
                        st.error(f'SHAPå€¤ã®å½¢çŠ¶ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚æœŸå¾…: {(len(X_sample), len(X_sample.columns))}, å®Ÿéš›: {shap_values_2d.shape}')
                        st.stop()
                    
                    # SHAP Summary Plot (using plotly)
                    shap_df = pd.DataFrame(shap_values_2d, columns=X_sample.columns)
                    
                    # Feature importance based on mean absolute SHAP values
                    shap_importance = pd.DataFrame({
                        'feature': shap_df.columns,
                        'mean_abs_shap': np.abs(shap_df).mean().values
                    }).sort_values('mean_abs_shap', ascending=False).head(15)
                    
                    fig_shap = px.bar(
                        shap_importance,
                        y='feature',
                        x='mean_abs_shap',
                        orientation='h',
                        title='SHAP Feature Importance (Mean |SHAP value|)'
                    )
                    st.plotly_chart(fig_shap, use_container_width=True)
                    
                    # SHAP values for individual prediction
                    st.write('**å€‹åˆ¥äºˆæ¸¬ã®SHAPå€¤**')
                    sample_idx = st.slider(
                        'ã‚µãƒ³ãƒ—ãƒ«é¸æŠ:', 
                        min_value=0, 
                        max_value=len(X_sample)-1, 
                        value=0
                    )
                    
                    sample_shap = shap_values_2d[sample_idx]
                    sample_features = X_sample.iloc[sample_idx]
                    
                    sample_shap_df = pd.DataFrame({
                        'feature': X_sample.columns,
                        'feature_value': sample_features.values,
                        'shap_value': sample_shap
                    }).sort_values('shap_value', key=abs, ascending=False).head(15)
                    
                    fig_sample = px.bar(
                        sample_shap_df,
                        y='feature',
                        x='shap_value',
                        orientation='h',
                        title=f'SHAP Values for Sample {sample_idx}',
                        color='shap_value',
                        color_continuous_scale='RdBu'
                    )
                    st.plotly_chart(fig_sample, use_container_width=True)
                    
                    # Store SHAP results
                    st.session_state[f'shap_values_{selected_model}'] = shap_values_2d
                    st.session_state[f'shap_sample_{selected_model}'] = X_sample
        
        # Model download
        st.write('#### ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜')
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Save model
            model_bytes = BytesIO()
            joblib.dump(result['model'], model_bytes)
            model_bytes.seek(0)
            
            st.download_button(
                'ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.pkl)',
                data=model_bytes.getvalue(),
                file_name=f'{selected_model}_model.pkl',
                mime='application/octet-stream'
            )
        
        with col2:
            # Save preprocessing pipeline
            preprocessing_bytes = BytesIO()
            joblib.dump(st.session_state['automl_preprocessing'], preprocessing_bytes)
            preprocessing_bytes.seek(0)
            
            st.download_button(
                'å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.pkl)',
                data=preprocessing_bytes.getvalue(),
                file_name='preprocessing_pipeline.pkl',
                mime='application/octet-stream'
            )

# -------- Merge & Export --------
with tabs[6]:
    st.subheader('ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸ & ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ')
    all_files = list(file_store.keys())
    
    # Export engineered features
    if 'engineered_df' in st.session_state:
        st.write('### ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ')
        engineered_df = st.session_state['engineered_df']
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric('å…ƒã®åˆ—æ•°', len(sampled_pd.columns))
        with col2:
            st.metric('æ–°ã—ã„åˆ—æ•°', len(engineered_df.columns))
            
        # Show feature creation summary
        if 'feature_info' in st.session_state:
            feature_info = st.session_state['feature_info']
            st.write('### ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡ã®è©³ç´°')
            for feature_type, info in feature_info.items():
                if feature_type == 'onehot':
                    for col, details in info.items():
                        st.write(f"**{col}** â†’ {len(details['encoded_cols'])}å€‹ã®OneHotç‰¹å¾´é‡")
                else:
                    st.write(f"**{feature_type}**: {len(info)}å€‹ã®ç‰¹å¾´é‡")
        
        # Download options
        csv_engineered = engineered_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            'ğŸ”½ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)',
            data=csv_engineered,
            file_name=f'engineered_{selected_file}',
            mime='text/csv'
        )
        
        # Export with clusters if available
        if 'clustered_df' in st.session_state:
            clustered_df = st.session_state['clustered_df']
            csv_clustered = clustered_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                'ğŸ”½ ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)',
                data=csv_clustered,
                file_name=f'clustered_{selected_file}',
                mime='text/csv'
            )
    
    st.write('---')
    
    # File merging
    if len(all_files) < 2:
        st.info('2ã¤ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãƒãƒ¼ã‚¸ã§ãã¾ã™')
    else:
        st.write('### ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸')
        left = st.selectbox('Left file', options=all_files)
        right = st.selectbox('Right file', options=[f for f in all_files if f!=left])
        l_pl, l_pd = file_store[left]
        r_pl, r_pd = file_store[right]
        common = list(set(l_pl.columns) & set(r_pl.columns))
        st.write('å…±é€šåˆ—å€™è£œ:', common)
        left_keys = st.multiselect('Left key(s)', options=list(l_pl.columns), default=common[:1] if common else [])
        right_keys = st.multiselect('Right key(s)', options=list(r_pl.columns), default=common[:1] if common else [])
        how = st.selectbox('Join type', ['inner','left','right','outer'])
        if st.button('ãƒãƒ¼ã‚¸å®Ÿè¡Œ'):
            if not left_keys or not right_keys or len(left_keys)!=len(right_keys):
                st.error('å·¦ã¨å³ã§å¯¾å¿œã™ã‚‹ã‚­ãƒ¼ã‚’åŒæ•°é¸æŠã—ã¦ãã ã•ã„')
            else:
                with st.spinner('ãƒãƒ¼ã‚¸ä¸­...'):
                    try:
                        merged = l_pd.merge(r_pd, left_on=left_keys, right_on=right_keys, how=how, suffixes=('_l','_r'))
                        st.success('ãƒãƒ¼ã‚¸å®Œäº†')
                        st.dataframe(merged.head(500))
                        csv = merged.to_csv(index=False).encode('utf-8')
                        st.download_button('ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (merged.csv)', data=csv, file_name='merged.csv', mime='text/csv')
                    except Exception as e:
                        st.error(f'ãƒãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼: {e}')

# -------- Report --------
with tabs[7]:
    st.subheader('ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ')
    
    # Use engineered dataframe if available for report
    report_df = st.session_state.get('engineered_df', sampled_pd)
    report_types = st.session_state.get('engineered_types', types)
    
    title = st.text_input('ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«', value=f'EDA + AutoML Report - {selected_file}')
    
    # Enhanced description with feature engineering and AutoML info
    default_desc = 'ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ EDA Studio Pro ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚'
    
    if 'feature_info' in st.session_state:
        feature_info = st.session_state['feature_info']
        feature_count = sum(len(info) if isinstance(info, list) else len(info) for info in feature_info.values())
        default_desc += f' {feature_count}å€‹ã®æ–°ã—ã„ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚'
    
    if 'automl_results' in st.session_state:
        automl_results = st.session_state['automl_results']
        best_model = max(automl_results.items(), key=lambda x: x[1]['cv_score'])
        default_desc += f' AutoMLã«ã‚ˆã‚Š{len(automl_results)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒæ¯”è¼ƒã•ã‚Œã€æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¯{best_model[0]}ã§ã—ãŸï¼ˆCV Score: {best_model[1]["cv_score"]:.4f}ï¼‰ã€‚'
    
    desc = st.text_area('èª¬æ˜ (ãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ã«å…¥ã‚‹è¦ç´„)', value=default_desc)
    
    # Plot selection for report
    plot_options = st.multiselect(
        'ãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã‚‹å›³è¡¨:',
        [
            'æ•°å€¤åˆ—ã®åˆ†å¸ƒ',
            'ç›¸é–¢è¡Œåˆ—',
            'PCAçµæœ',
            'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ',
            'AutoMLæ€§èƒ½æ¯”è¼ƒ',
            'ç‰¹å¾´é‡é‡è¦åº¦',
            'SHAPåˆ†æçµæœ'
        ],
        default=['æ•°å€¤åˆ—ã®åˆ†å¸ƒ', 'ç›¸é–¢è¡Œåˆ—', 'AutoMLæ€§èƒ½æ¯”è¼ƒ']
    )
    
    imgs = []
    
    if st.button('ãƒ¬ãƒãƒ¼ãƒˆç”¨ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆ'): 
        with st.spinner('ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...'):
            numeric_cols = [c for c,t in report_types.items() if t=='numeric']
            
            if 'æ•°å€¤åˆ—ã®åˆ†å¸ƒ' in plot_options and numeric_cols:
                top_num = numeric_cols[0]
                fig1 = px.histogram(report_df, x=top_num, nbins=60, title=f'Distribution: {top_num}')
                img1 = plotly_to_png(fig1)
                imgs.append((img1, f'Distribution of {top_num}'))
            
            if 'ç›¸é–¢è¡Œåˆ—' in plot_options and len(numeric_cols) >= 2:
                corr = report_df[numeric_cols[:10]].corr()  # Limit to top 10 for readability
                fig2 = px.imshow(corr, text_auto='.2f', title='Correlation matrix')
                img2 = plotly_to_png(fig2)
                imgs.append((img2, 'Correlation matrix (top 10 numeric features)'))
            
            if 'PCAçµæœ' in plot_options and len(numeric_cols) >= 2:
                pc_res = run_pca(report_df, numeric_cols[:10], 3)
                if pc_res is not None:
                    comp_df, var = pc_res
                    comp_df_display = comp_df.copy()
                    comp_df_display['index'] = np.arange(len(comp_df_display))
                    fig3 = px.scatter(
                        comp_df_display, 
                        x='PC1', y='PC2', 
                        title=f'PCA: PC1 ({var[0]:.1%}) vs PC2 ({var[1]:.1%})'
                    )
                    img3 = plotly_to_png(fig3)
                    imgs.append((img3, 'Principal Component Analysis'))
            
            if 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ' in plot_options and 'clustered_df' in st.session_state:
                clustered_df = st.session_state['clustered_df']
                cluster_col = 'cluster'
                if cluster_col in clustered_df.columns:
                    cluster_summary = clustered_df.groupby(cluster_col).size().reset_index(name='count')
                    fig4 = px.bar(cluster_summary, x=cluster_col, y='count', title='Cluster Distribution')
                    img4 = plotly_to_png(fig4)
                    imgs.append((img4, 'Cluster analysis results'))
            
            if 'AutoMLæ€§èƒ½æ¯”è¼ƒ' in plot_options and 'automl_results' in st.session_state:
                automl_results = st.session_state['automl_results']
                task_type = st.session_state['automl_task_type']
                
                # Create performance comparison chart
                model_names = list(automl_results.keys())
                cv_scores = [result['cv_score'] for result in automl_results.values()]
                
                fig5 = px.bar(
                    x=model_names,
                    y=cv_scores,
                    title='AutoML Model Performance Comparison (CV Score)'
                )
                img5 = plotly_to_png(fig5)
                imgs.append((img5, 'AutoML model performance comparison'))
            
            if 'ç‰¹å¾´é‡é‡è¦åº¦' in plot_options and 'automl_results' in st.session_state:
                # Use best model's feature importance
                automl_results = st.session_state['automl_results']
                best_model = max(automl_results.items(), key=lambda x: x[1]['cv_score'])
                
                if best_model[1]['feature_importance'] is not None:
                    importance_df = best_model[1]['feature_importance'].head(15)
                    fig6 = px.bar(
                        importance_df,
                        y='feature',
                        x='importance',
                        orientation='h',
                        title=f'{best_model[0]} - Top 15 Feature Importance'
                    )
                    img6 = plotly_to_png(fig6)
                    imgs.append((img6, f'Feature importance from best model: {best_model[0]}'))
            
            st.success('ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†')
            st.session_state['report_imgs'] = imgs
    
    # Store images in session state to persist them
    if 'report_imgs' in st.session_state:
        imgs = st.session_state['report_imgs']
        
    if imgs:
        st.write(f'ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆæ•°: {len(imgs)}')
        
        if st.button('PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ'):
            with st.spinner('PDFä½œæˆä¸­...'):
                pdf_bytes = create_pdf_report(title, desc, imgs)
                st.download_button('PDF ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰', data=pdf_bytes, file_name='eda_automl_report.pdf', mime='application/pdf')
    
    # Feature engineering + AutoML summary
    if 'feature_info' in st.session_state or 'automl_results' in st.session_state:
        st.write('---')
        st.write('### è§£æã‚µãƒãƒª')
        
        summary_text = "# EDA + AutoML Analysis Summary\n\n"
        
        # Feature engineering summary
        if 'feature_info' in st.session_state:
            feature_info = st.session_state['feature_info']
            
            summary_text += "## Feature Engineering Summary\n\n"
            for feature_type, info in feature_info.items():
                if feature_type == 'onehot':
                    summary_text += f"### OneHot Encoding\n"
                    for col, details in info.items():
                        summary_text += f"- {col}: {details['original_unique']} categories â†’ {len(details['encoded_cols'])} binary features\n"
                else:
                    feature_count = len(info) if isinstance(info, list) else len(info)
                    summary_text += f"### {feature_type.title()}: {feature_count} features created\n"
        
        # AutoML summary
        if 'automl_results' in st.session_state:
            automl_results = st.session_state['automl_results']
            task_type = st.session_state['automl_task_type']
            
            summary_text += f"\n## AutoML Results Summary\n\n"
            summary_text += f"**Task Type**: {task_type}\n"
            summary_text += f"**Models Compared**: {len(automl_results)}\n\n"
            
            # Best model
            best_model = max(automl_results.items(), key=lambda x: x[1]['cv_score'])
            summary_text += f"**Best Model**: {best_model[0]}\n"
            summary_text += f"**Best CV Score**: {best_model[1]['cv_score']:.4f}\n\n"
            
            # All model results
            summary_text += "### Model Comparison\n\n"
            for model_name, result in automl_results.items():
                summary_text += f"**{model_name}**\n"
                summary_text += f"- CV Score: {result['cv_score']:.4f}\n"
                summary_text += f"- Training Time: {result['training_time']:.1f}s\n"
                
                if task_type == 'classification':
                    summary_text += f"- Test Accuracy: {result['test_results']['accuracy']:.4f}\n"
                    summary_text += f"- Test F1: {result['test_results']['f1']:.4f}\n"
                else:
                    summary_text += f"- Test RMSE: {result['test_results']['rmse']:.4f}\n"
                    summary_text += f"- Test RÂ²: {result['test_results']['r2']:.4f}\n"
                
                summary_text += "\n"
        
        st.markdown(summary_text)
        
        # Download comprehensive report
        summary_bytes = summary_text.encode('utf-8')
        st.download_button(
            'å®Œå…¨è§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Markdown)',
            data=summary_bytes,
            file_name='complete_analysis_report.md',
            mime='text/markdown'
        )

st.success('EDA Studio Pro + AutoML ã‚’èµ·å‹•ã—ã¾ã—ãŸ')